{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jenny005/Langgraph-Tutorial/blob/main/Copy_of_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68e21aa1",
      "metadata": {
        "id": "68e21aa1"
      },
      "source": [
        "# Evaluating Agents\n",
        "\n",
        "We have an email assistant that uses a router to triage emails and then passes the email to the agent for response generation. How can we be sure that it will work well in production? This is why testing is important: it guides our decisions about our agent architecture with quantifiable metrics like response quality, token usage, latency, or triage accuracy. [LangSmith](https://docs.smith.langchain.com/) offers two primary ways to test agents.\n",
        "\n",
        "![overview-img](https://github.com/langchain-ai/agents-from-scratch/blob/main/notebooks/img/overview_eval.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r /content/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bh6s9hIuDKe3",
        "outputId": "f499b569-c84f-4e27-f407-779ae43fb516"
      },
      "id": "bh6s9hIuDKe3",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 1)) (4.14.1)\n",
            "Collecting typing (from -r /content/requirements.txt (line 2))\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/78.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 3)) (0.6.6)\n",
            "Collecting langchain_openai (from -r /content/requirements.txt (line 4))\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 5)) (0.3.74)\n",
            "Collecting langgraph-checkpoint-mongodb (from -r /content/requirements.txt (line 6))\n",
            "  Downloading langgraph_checkpoint_mongodb-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain_community (from -r /content/requirements.txt (line 7))\n",
            "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 8)) (1.1.1)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (from -r /content/requirements.txt (line 9)) (0.2.65)\n",
            "Collecting pymongo (from -r /content/requirements.txt (line 10))\n",
            "  Downloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting wikipedia (from -r /content/requirements.txt (line 11))\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trustcall (from -r /content/requirements.txt (line 12))\n",
            "  Downloading trustcall-0.0.39-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting langchain-mcp-adapters (from -r /content/requirements.txt (line 13))\n",
            "  Downloading langchain_mcp_adapters-0.1.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r /content/requirements.txt (line 3)) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r /content/requirements.txt (line 3)) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r /content/requirements.txt (line 3)) (0.2.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r /content/requirements.txt (line 3)) (2.11.7)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph->-r /content/requirements.txt (line 3)) (3.5.0)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.99.9 in /usr/local/lib/python3.12/dist-packages (from langchain_openai->-r /content/requirements.txt (line 4)) (1.100.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain_openai->-r /content/requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain_core->-r /content/requirements.txt (line 5)) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_core->-r /content/requirements.txt (line 5)) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain_core->-r /content/requirements.txt (line 5)) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain_core->-r /content/requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain_core->-r /content/requirements.txt (line 5)) (25.0)\n",
            "Collecting langchain-mongodb>=0.6.1 (from langgraph-checkpoint-mongodb->-r /content/requirements.txt (line 6))\n",
            "  Downloading langchain_mongodb-0.7.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (2.32.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (3.12.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community->-r /content/requirements.txt (line 7))\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community->-r /content/requirements.txt (line 7)) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (2.2.2)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance->-r /content/requirements.txt (line 9)) (15.0.1)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->-r /content/requirements.txt (line 10))\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting dydantic<1.0.0,>=0.0.8 (from trustcall->-r /content/requirements.txt (line 12))\n",
            "  Downloading dydantic-0.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: mcp>=1.9.2 in /usr/local/lib/python3.12/dist-packages (from langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (1.13.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community->-r /content/requirements.txt (line 7)) (1.20.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance->-r /content/requirements.txt (line 9)) (2.7)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance->-r /content/requirements.txt (line 9)) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance->-r /content/requirements.txt (line 9)) (2025.8.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/requirements.txt (line 7))\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/requirements.txt (line 7))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core->-r /content/requirements.txt (line 5)) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community->-r /content/requirements.txt (line 7)) (0.3.9)\n",
            "Collecting lark<2.0.0,>=1.1.9 (from langchain-mongodb>=0.6.1->langgraph-checkpoint-mongodb->-r /content/requirements.txt (line 6))\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph->-r /content/requirements.txt (line 3)) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r /content/requirements.txt (line 3)) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r /content/requirements.txt (line 3)) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core->-r /content/requirements.txt (line 5)) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain_core->-r /content/requirements.txt (line 5)) (0.24.0)\n",
            "Requirement already satisfied: anyio>=4.5 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (4.10.0)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (4.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (3.0.2)\n",
            "Requirement already satisfied: starlette>=0.27 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (0.47.2)\n",
            "Requirement already satisfied: uvicorn>=0.31.1 in /usr/local/lib/python3.12/dist-packages (from mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (0.35.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai->-r /content/requirements.txt (line 4)) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai->-r /content/requirements.txt (line 4)) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai->-r /content/requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<2.0.0,>=1.99.9->langchain_openai->-r /content/requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance->-r /content/requirements.txt (line 9)) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance->-r /content/requirements.txt (line 9)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r /content/requirements.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r /content/requirements.txt (line 3)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langgraph->-r /content/requirements.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain_community->-r /content/requirements.txt (line 7)) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain_community->-r /content/requirements.txt (line 7)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain_community->-r /content/requirements.txt (line 7)) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community->-r /content/requirements.txt (line 7)) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain_openai->-r /content/requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance->-r /content/requirements.txt (line 9)) (2.22)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r /content/requirements.txt (line 3)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph->-r /content/requirements.txt (line 3)) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance->-r /content/requirements.txt (line 9)) (1.17.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community->-r /content/requirements.txt (line 7))\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn>=0.31.1->mcp>=1.9.2->langchain-mcp-adapters->-r /content/requirements.txt (line 13)) (8.2.1)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint_mongodb-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymongo-4.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trustcall-0.0.39-py3-none-any.whl (30 kB)\n",
            "Downloading langchain_mcp_adapters-0.1.9-py3-none-any.whl (15 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dydantic-0.0.8-py3-none-any.whl (8.6 kB)\n",
            "Downloading langchain_mongodb-0.7.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: typing, wikipedia\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26304 sha256=38b0e5f6e405a1fa1b1bf34cf72e25be077293d968a7ea0a3ea44fd95b9b5281\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/98/52/2bffe242a9a487f00886e43b8ed8dac46456702e11a0d6abef\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=7abc7f15c37a05ad1b08d5814c7f47cdd7a2fea5163bbe2af88ad04b94db354a\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/47/7c/a9688349aa74d228ce0a9023229c6c0ac52ca2a40fe87679b8\n",
            "Successfully built typing wikipedia\n",
            "Installing collected packages: typing, mypy-extensions, marshmallow, lark, dnspython, wikipedia, typing-inspect, pymongo, dydantic, dataclasses-json, langchain_openai, langchain-mcp-adapters, langchain-mongodb, langchain_community, trustcall, langgraph-checkpoint-mongodb\n",
            "Successfully installed dataclasses-json-0.6.7 dnspython-2.7.0 dydantic-0.0.8 langchain-mcp-adapters-0.1.9 langchain-mongodb-0.7.0 langchain_community-0.3.27 langchain_openai-0.3.30 langgraph-checkpoint-mongodb-0.2.0 lark-1.2.2 marshmallow-3.26.1 mypy-extensions-1.1.0 pymongo-4.14.1 trustcall-0.0.39 typing-3.7.4.3 typing-inspect-0.9.0 wikipedia-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "langgraph",
                  "typing"
                ]
              },
              "id": "9d1826d5daa24e3ba1254e8b134708e4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d7f7048",
      "metadata": {
        "id": "4d7f7048"
      },
      "source": [
        "#### Load Environment Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c47d4c3d",
      "metadata": {
        "id": "c47d4c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb05f620-8093-4ec3-8e97-fcd25be9e53d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from dotenv import load_dotenv  # load environment variables from a .env file into your program‚Äôs os.environ dictionary\n",
        "load_dotenv(\"/content/.env\", override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2005c34d",
      "metadata": {
        "id": "2005c34d"
      },
      "source": [
        "## How to run Evaluations\n",
        "\n",
        "#### Pytest / Vitest\n",
        "\n",
        "[Pytest](https://docs.pytest.org/en/stable/) and Vitest are well known to many developers as a powerful tools for writing tests within the Python and JavaScript ecosystems. LangSmith integrates with these frameworks to allow you to write and run tests that log results to LangSmith. For this notebook, we'll use Pytest.\n",
        "* Pytest is a great way to get started for developers who are already familiar with their framework.\n",
        "* Pytest is great for more complex evaluations, where each agent test case requires specific checks and success criteria that are harder to generalize.\n",
        "\n",
        "#### LangSmith Datasets\n",
        "\n",
        "You can also create a dataset [in LangSmith](https://docs.smith.langchain.com/evaluation) and run our assistant against the dataset using the LangSmith evaluate API.\n",
        "* LangSmith datasets are great for teams who are collaboratively building out their test suite.\n",
        "* You can leverage production traces, annotation queues, synthetic data generation, and more, to add examples to an ever-growing golden dataset.\n",
        "* LangSmith datasets are great when you can define evaluators that can be applied to every test case in the dataset (ex. similarity, exact match accuracy, etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b7c989",
      "metadata": {
        "id": "10b7c989"
      },
      "source": [
        "## Test Cases\n",
        "\n",
        "Testing often starts with defining the test cases, which can be a challenging process. In this case, we'll just define a set of example emails we want to handle along with a few things to test. You can see the test cases in `eval/email_dataset.py`, which contains the following:\n",
        "\n",
        "1. **Input Emails**: A collection of diverse email examples\n",
        "2. **Ground Truth Classifications**: `Respond`, `Notify`, `Ignore`\n",
        "3. **Expected Tool Calls**: Tools called for each email that requires a response\n",
        "4. **Response Criteria**: What makes a good response for emails requiring replies\n",
        "\n",
        "Note that we have both\n",
        "- End to end \"integration\" tests (e.g. Input Emails -> Agent -> Final Output vs Response Criteria)\n",
        "- Tests for specific steps in our workflow (e.g. Input Emails -> Agent -> Classification vs Ground Truth Classification)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/agents-from-scratch-main\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK52-iq1Cjb3",
        "outputId": "50ecae88-028e-42dd-9e53-bcbfd708a14d"
      },
      "id": "LK52-iq1Cjb3",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/agents-from-scratch-main/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkU3gILYCu4F",
        "outputId": "e2b91932-49a0-41c5-c166-514ed94da568"
      },
      "id": "KkU3gILYCu4F",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/agents-from-scratch-main/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install html2text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOIMctlnC_J2",
        "outputId": "b5853060-da64-40e7-c11e-1c17ae79722e"
      },
      "id": "nOIMctlnC_J2",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: html2text in /usr/local/lib/python3.12/dist-packages (2025.4.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f8fdc2b8",
      "metadata": {
        "id": "f8fdc2b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3583bbb-8b92-4c5a-f904-12371b7381aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
            "Expected Triage Output: respond\n",
            "Expected Tool Calls: ['write_email', 'done']\n",
            "Response Criteria: \n",
            "‚Ä¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls, triage_outputs_list, response_criteria_list\n",
        "\n",
        "test_case_ix = 0\n",
        "\n",
        "print(\"Email Input:\", email_inputs[test_case_ix])\n",
        "print(\"Expected Triage Output:\", triage_outputs_list[test_case_ix])\n",
        "print(\"Expected Tool Calls:\", expected_tool_calls[test_case_ix])\n",
        "print(\"Response Criteria:\", response_criteria_list[test_case_ix])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2337bd7c",
      "metadata": {
        "id": "2337bd7c"
      },
      "source": [
        "## Pytest Example\n",
        "\n",
        "Let's take a look at how we can write a test for a specific part of our workflow with Pytest. We will test whether our `email_assistant` makes the right tool calls when responding to the emails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ae92fe30",
      "metadata": {
        "id": "ae92fe30"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "from email_assistant.eval.email_dataset import email_inputs, expected_tool_calls\n",
        "from email_assistant.utils import format_messages_string\n",
        "from email_assistant.email_assistant import email_assistant\n",
        "from email_assistant.utils import extract_tool_calls\n",
        "\n",
        "from langsmith import testing as t\n",
        "\n",
        "@pytest.mark.langsmith\n",
        "@pytest.mark.parametrize(\n",
        "    \"email_input, expected_calls\",\n",
        "    [   # Pick some examples with e-mail reply expected\n",
        "        (email_inputs[0],expected_tool_calls[0]),\n",
        "        (email_inputs[3],expected_tool_calls[3]),\n",
        "    ],\n",
        ")\n",
        "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
        "    \"\"\"Test if email processing contains expected tool calls.\n",
        "\n",
        "    This test confirms that all expected tools are called during email processing,\n",
        "    but does not check the order of tool invocations or the number of invocations\n",
        "    per tool. Additional checks for these aspects could be added if desired.\n",
        "    \"\"\"\n",
        "    # Run the email assistant\n",
        "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
        "    result = email_assistant.invoke({\"messages\": messages})\n",
        "\n",
        "    # Extract tool calls from messages list\n",
        "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
        "\n",
        "    # Check if all expected tool calls are in the extracted ones\n",
        "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
        "\n",
        "    t.log_outputs({\n",
        "                \"missing_calls\": missing_calls,\n",
        "                \"extracted_tool_calls\": extracted_tool_calls,\n",
        "                \"response\": format_messages_string(result['messages'])\n",
        "            })\n",
        "\n",
        "    # Test passes if no expected calls are missing\n",
        "    assert len(missing_calls) == 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "700aba2a",
      "metadata": {
        "id": "700aba2a"
      },
      "source": [
        "You'll notice a few things.\n",
        "- To [run with Pytest and log test results to LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), we only need to add the `@pytest.mark.langsmith ` decorator to our function and place it in a file, as you see in `notebooks/test_tools.py`. This will log the test results to LangSmith.\n",
        "- Second, we can pass dataset examples to the test function as shown [here](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`.\n",
        "\n",
        "#### Running Pytest\n",
        "We can run the test from the command line. We've defined the above code in a python file. From the project root, run:\n",
        "\n",
        "`! LANGSMITH_TEST_SUITE='Email assistant: Test Tools For Interrupt'  pytest notebooks/test_tools.py`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53165e98",
      "metadata": {
        "id": "53165e98"
      },
      "source": [
        "#### Viewing Experiment Result\n",
        "\n",
        "We can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith. The `log_outputs` are passed to the `Outputs` column and function arguments are passed to the `Inputs` column. Each input passed in `@pytest.mark.parametrize(` is a separate row logged to the `LANGSMITH_TEST_SUITE` project name in LangSmith, which is found under `Datasets & Experiments`.\n",
        "\n",
        "![Test Results](https://github.com/langchain-ai/agents-from-scratch/blob/main/notebooks/img/test_result.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd325e27",
      "metadata": {
        "id": "fd325e27"
      },
      "source": [
        "## LangSmith Datasets Example\n",
        "\n",
        "![overview-img](https://github.com/langchain-ai/agents-from-scratch/blob/main/notebooks/img/eval_detail.png?raw=1)\n",
        "\n",
        "Let's take a look at how we can run evaluations with LangSmith datasets. In the previous example with Pytest, we evaluated the tool calling accuracy of the email assistant. Now, the dataset that we're going to evaluate here is specifically for the triage step of the email assistant, in classifying whether an email requires a response.\n",
        "\n",
        "#### Dataset Definition\n",
        "\n",
        "We can [create a dataset in LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) with the LangSmith SDK. The below code creates a dataset with the test cases in the `eval/email_dataset.py` file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7ea997ac",
      "metadata": {
        "id": "7ea997ac"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "from email_assistant.eval.email_dataset import examples_triage\n",
        "\n",
        "# Initialize LangSmith client\n",
        "client = Client()\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = \"E-mail Triage Evaluation\"\n",
        "\n",
        "# Create dataset if it doesn't exist\n",
        "if not client.has_dataset(dataset_name=dataset_name):\n",
        "    dataset = client.create_dataset(\n",
        "        dataset_name=dataset_name,\n",
        "        description=\"A dataset of e-mails and their triage decisions.\"\n",
        "    )\n",
        "    # Add examples to the dataset\n",
        "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2df606",
      "metadata": {
        "id": "0b2df606"
      },
      "source": [
        "#### Target Function\n",
        "\n",
        "The dataset has the following structure, with an e-mail input and a ground truth triage classification for the e-mail as output:\n",
        "\n",
        "```\n",
        "examples_triage = [\n",
        "  {\n",
        "      \"inputs\": {\"email_input\": email_input_1},\n",
        "      \"outputs\": {\"classification\": triage_output_1},   # NOTE: This becomes the reference_output in the created dataset\n",
        "  }, ...\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f7d7e83f-3006-4386-9230-786545c7b1a1",
      "metadata": {
        "id": "f7d7e83f-3006-4386-9230-786545c7b1a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a67e3f18-20f4-4cc4-8666-ba34c77eb25e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Example Input (inputs): {'email_input': {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}}\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset Example Input (inputs):\", examples_triage[0]['inputs'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f292f070-7af6-4370-9338-e90bfd6b3d42",
      "metadata": {
        "id": "f292f070-7af6-4370-9338-e90bfd6b3d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa4ec8db-6d44-4425-ced6-5dc5a3f56619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Example Reference Output (reference_outputs): {'classification': 'respond'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Dataset Example Reference Output (reference_outputs):\", examples_triage[0]['outputs'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8290e820",
      "metadata": {
        "id": "8290e820"
      },
      "source": [
        "We define a function that takes the dataset inputs and passes them to our email assistant. The LangSmith [evaluate API](https://docs.smith.langchain.com/evaluation) passes the `inputs` dict to this function. This function then returns a dict with the agent's output. Because we are evaluating the triage step, we only need to return the classification decision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0b9d1ded",
      "metadata": {
        "id": "0b9d1ded"
      },
      "outputs": [],
      "source": [
        "def target_email_assistant(inputs: dict) -> dict:\n",
        "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
        "    response = email_assistant.nodes['triage_router'].invoke({\"email_input\": inputs[\"email_input\"]})\n",
        "    return {\"classification_decision\": response.update['classification_decision']}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba6ec4c",
      "metadata": {
        "id": "5ba6ec4c"
      },
      "source": [
        "#### Evaluator Function\n",
        "\n",
        "Now, we create an evaluator function. What do we want to evaluate? We have reference outputs in our dataset and agent outputs defined in the functions above.\n",
        "\n",
        "* Reference outputs: `\"reference_outputs\": {\"classification\": triage_output_1} ...`\n",
        "* Agent outputs: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n",
        "\n",
        "We want to evaluate if the agent's output matches the reference output. So we simply need a an evaluator function that compares the two, where `outputs` is the agent's output and `reference_outputs` is the reference output from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4fee7532",
      "metadata": {
        "id": "4fee7532"
      },
      "outputs": [],
      "source": [
        "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
        "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
        "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50fd2de9",
      "metadata": {
        "id": "50fd2de9"
      },
      "source": [
        "### Running Evaluation\n",
        "\n",
        "Now, the question is: how are these things hooked together? The evaluate API takes care of it for us. It passes the `inputs` dict from our dataset the target function. It passes the `reference_outputs` dict from our dataset to the evaluator function. And it passes the `outputs` of our agent to the evaluator function.\n",
        "\n",
        "Note this is similar to what we did with Pytest: in Pytest, we passed in the dataset example inputs and reference outputs to the test function with `@pytest.mark.parametrize`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6807306d",
      "metadata": {
        "id": "6807306d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396,
          "referenced_widgets": [
            "abbc9436fe93409bb5125db843a41a4b",
            "23f4ed3dc5064995a1b48b2636c5418c",
            "a0261e58d80042a0a741c9d6ef484231",
            "e14a250448a3498380dc8453fd744e36",
            "e85baad46cc646b49a762bf6d3449944",
            "6dddf0af97bf49bdacb4806a62a547c8",
            "52192df736a4414c89114d9341e531ff",
            "8b7ca91e6da84aaea20aa78f3d5b0943",
            "66d7a506fe7e40c78666ca1ff7da4ac6",
            "21b3156ec60b429cb36b6134bdd2bbe2",
            "6e1f85bb34344bc7a19f326ae3afba90"
          ]
        },
        "outputId": "61673c5f-224d-414c-9fb6-ff0742ff3bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'E-mail assistant workflow-f0e5b196' at:\n",
            "https://smith.langchain.com/o/e1387bdc-c442-4277-83bf-9ef209d9a54e/datasets/80a451ca-9a0f-4a7e-9af0-79819ee8ccf5/compare?selectedSessions=5ed921f8-6396-4e67-9c0f-d68f1dd15535\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abbc9436fe93409bb5125db843a41a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìß Classification: RESPOND - This email requires a response\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üîî Classification: NOTIFY - This email contains important information\n",
            "üö´ Classification: IGNORE - This email can be safely ignored\n",
            "üîî Classification: NOTIFY - This email contains important information\n",
            "üö´ Classification: IGNORE - This email can be safely ignored\n",
            "üö´ Classification: IGNORE - This email can be safely ignored\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üîî Classification: NOTIFY - This email contains important information\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üìß Classification: RESPOND - This email requires a response\n",
            "üîî Classification: NOTIFY - This email contains important information\n",
            "üîî Classification: NOTIFY - This email contains important information\n"
          ]
        }
      ],
      "source": [
        "# Set to true if you want to kick off evaluation\n",
        "run_expt = True\n",
        "if run_expt:\n",
        "    experiment_results_workflow = client.evaluate(\n",
        "        # Run agent\n",
        "        target_email_assistant,\n",
        "        # Dataset name\n",
        "        data=dataset_name,\n",
        "        # Evaluator\n",
        "        evaluators=[classification_evaluator],\n",
        "        # Name of the experiment\n",
        "        experiment_prefix=\"E-mail assistant workflow\",\n",
        "        # Number of concurrent evaluations\n",
        "        max_concurrency=2,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76baff88",
      "metadata": {
        "id": "76baff88"
      },
      "source": [
        "We can view the results from both experiments in the LangSmith UI.\n",
        "\n",
        "![Test Results](https://github.com/langchain-ai/agents-from-scratch/blob/main/notebooks/img/eval.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5146b52",
      "metadata": {
        "id": "c5146b52"
      },
      "source": [
        "## LLM-as-Judge Evaluation\n",
        "\n",
        "We've shown unit tests for the triage step (using evaluate()) and tool calling (using Pytest).\n",
        "\n",
        "We'll showcase how you could use an LLM as a judge to evaluate our agent's execution against a set of success criteria.\n",
        "\n",
        "![types](https://github.com/langchain-ai/agents-from-scratch/blob/main/notebooks/img/eval_types.png?raw=1)\n",
        "\n",
        "First, we define a structured output schema for our LLM grader that contains a grade and justification for the grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e1d342b8",
      "metadata": {
        "id": "e1d342b8"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "class CriteriaGrade(BaseModel):\n",
        "    \"\"\"Score the response against specific criteria.\"\"\"\n",
        "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
        "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
        "\n",
        "# Create a global LLM for evaluation to avoid recreating it for each test\n",
        "criteria_eval_llm = init_chat_model(\"openai:gpt-4o\")\n",
        "criteria_eval_structured_llm = criteria_eval_llm.with_structured_output(CriteriaGrade)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bec02b18",
      "metadata": {
        "id": "bec02b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d855c009-d806-4d21-95f1-c7fd7aca3e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Email Input: {'author': 'Alice Smith <alice.smith@company.com>', 'to': 'Lance Martin <lance@company.com>', 'subject': 'Quick question about API documentation', 'email_thread': \"Hi Lance,\\n\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\n\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\n\\nThanks!\\nAlice\"}\n",
            "Success Criteria: \n",
            "‚Ä¢ Send email with write_email tool call to acknowledge the question and confirm it will be investigated  \n",
            "\n"
          ]
        }
      ],
      "source": [
        "email_input = email_inputs[0]\n",
        "print(\"Email Input:\", email_input)\n",
        "success_criteria = response_criteria_list[0]\n",
        "print(\"Success Criteria:\", success_criteria)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38390ccd",
      "metadata": {
        "id": "38390ccd"
      },
      "source": [
        "Our Email Assistant is invoked with the email input and the response is formatted into a string. These are all then passed to the LLM grader to receive a grade and justification for the grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "cbff28fc",
      "metadata": {
        "id": "cbff28fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c253e870-2ad4-400d-e3c7-e75e7d73bc4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìß Classification: RESPOND - This email requires a response\n"
          ]
        }
      ],
      "source": [
        "response = email_assistant.invoke({\"email_input\": email_input})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "d64619fb",
      "metadata": {
        "id": "d64619fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db1ba606-e7bb-4e24-fd24-0bbda39bdff7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CriteriaGrade(justification='The assistant\\'s response includes a write_email tool call, which satisfies the requirement to acknowledge the question about the API documentation. The email content specifically states: \"Thank you for bringing this to my attention. I\\'ll investigate whether the /auth/refresh and /auth/validate endpoints were intentionally omitted from the API documentation or if an update is needed.\" This confirms that the question will be investigated. Therefore, the criteria have been met.', grade=True)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from email_assistant.eval.prompts import RESPONSE_CRITERIA_SYSTEM_PROMPT\n",
        "\n",
        "all_messages_str = format_messages_string(response['messages'])\n",
        "eval_result = criteria_eval_structured_llm.invoke([\n",
        "        {\"role\": \"system\",\n",
        "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\",\n",
        "            \"content\": f\"\"\"\\n\\n Response criteria: {success_criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
        "    ])\n",
        "\n",
        "eval_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "64275647-6fdb-4bf3-806b-4dbc770cbd6f",
      "metadata": {
        "id": "64275647-6fdb-4bf3-806b-4dbc770cbd6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "36ab6766-b1f7-4739-ddcd-26561afb6c88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"You are evaluating an email assistant that works on behalf of a user.\\n\\nYou will see a sequence of messages, starting with an email sent to the user. \\n\\nYou will then see the assistant's response to this email on behalf of the user, which includes any tool calls made (e.g., write_email, schedule_meeting, check_calendar_availability, done).\\n\\nYou will also see a list of criteria that the assistant's response must meet.\\n\\nYour job is to evaluate if the assistant's response meets ALL the criteria bullet points provided.\\n\\nIMPORTANT EVALUATION INSTRUCTIONS:\\n1. The assistant's response is formatted as a list of messages.\\n2. The response criteria are formatted as bullet points (‚Ä¢)\\n3. You must evaluate the response against EACH bullet point individually\\n4. ALL bullet points must be met for the response to receive a 'True' grade\\n5. For each bullet point, cite specific text from the response that satisfies or fails to satisfy it\\n6. Be objective and rigorous in your evaluation\\n7. In your justification, clearly indicate which criteria were met and which were not\\n7. If ANY criteria are not met, the overall grade must be 'False'\\n\\nYour output will be used for automated testing, so maintain a consistent evaluation approach.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "RESPONSE_CRITERIA_SYSTEM_PROMPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7994952c",
      "metadata": {
        "id": "7994952c"
      },
      "source": [
        "We can see that the LLM grader returns an eval result with a schema matching our `CriteriaGrade` base model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b44111d",
      "metadata": {
        "id": "0b44111d"
      },
      "source": [
        "## Running against a Larger Test Suite\n",
        "Now that we've seen how to evaluate our agent using Pytest and evaluate(), and seen an example of using an LLM as a judge, we can use evaluations over a bigger test suite to get a better sense of how our agent performs over a wider variety of examples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9280d5ae-3070-4131-8763-454073176081",
      "metadata": {
        "id": "9280d5ae-3070-4131-8763-454073176081"
      },
      "source": [
        "Let's run our email_assistant against a larger test suite.\n",
        "```\n",
        "! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response Interrupt' LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
        "```\n",
        "\n",
        "In `test_response.py`, you can see a few things.\n",
        "\n",
        "We pass our dataset examples into functions that will run pytest and log to our `LANGSMITH_TEST_SUITE`:\n",
        "\n",
        "```\n",
        "# Reference output key\n",
        "@pytest.mark.langsmith(output_keys=[\"criteria\"])\n",
        "# Variable names and a list of tuples with the test cases\n",
        "# Each test case is (email_input, email_name, criteria, expected_calls)\n",
        "@pytest.mark.parametrize(\"email_input,email_name,criteria,expected_calls\",create_response_test_cases())\n",
        "def test_response_criteria_evaluation(email_input, email_name, criteria, expected_calls):\n",
        "```\n",
        "\n",
        "We use LLM-as-judge with a grading schema:\n",
        "```\n",
        "class CriteriaGrade(BaseModel):\n",
        "    \"\"\"Score the response against specific criteria.\"\"\"\n",
        "    grade: bool = Field(description=\"Does the response meet the provided criteria?\")\n",
        "    justification: str = Field(description=\"The justification for the grade and score, including specific examples from the response.\")\n",
        "```\n",
        "\n",
        "\n",
        "We evaluate the agent response relative to the criteria:\n",
        "```\n",
        "    # Evaluate against criteria\n",
        "    eval_result = criteria_eval_structured_llm.invoke([\n",
        "        {\"role\": \"system\",\n",
        "            \"content\": RESPONSE_CRITERIA_SYSTEM_PROMPT},\n",
        "        {\"role\": \"user\",\n",
        "            \"content\": f\"\"\"\\n\\n Response criteria: {criteria} \\n\\n Assistant's response: \\n\\n {all_messages_str} \\n\\n Evaluate whether the assistant's response meets the criteria and provide justification for your evaluation.\"\"\"}\n",
        "    ])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca836fbf",
      "metadata": {
        "id": "ca836fbf"
      },
      "source": [
        "Now let's take a look at this experiment in the LangSmith UI and look into what our agent did well, and what it could improve on.\n",
        "\n",
        "#### Getting Results\n",
        "\n",
        "We can also get the results of the evaluation by reading the tracing project associated with our experiment. This is great for creating custom visualizations of our agent's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "70b655f8",
      "metadata": {
        "lines_to_next_cell": 0,
        "id": "70b655f8"
      },
      "outputs": [],
      "source": [
        "# TODO: Copy your experiment name here\n",
        "experiment_name = \"email_assistant:8286b3b8\"\n",
        "# Set this to load expt results\n",
        "load_expt = False\n",
        "if load_expt:\n",
        "    email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
        "    print(\"Latency p50:\", email_assistant_experiment_results.latency_p50)\n",
        "    print(\"Latency p99:\", email_assistant_experiment_results.latency_p99)\n",
        "    print(\"Token Usage:\", email_assistant_experiment_results.total_tokens)\n",
        "    print(\"Feedback Stats:\", email_assistant_experiment_results.feedback_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ccdfaa6",
      "metadata": {
        "id": "0ccdfaa6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "abbc9436fe93409bb5125db843a41a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_23f4ed3dc5064995a1b48b2636c5418c",
              "IPY_MODEL_a0261e58d80042a0a741c9d6ef484231",
              "IPY_MODEL_e14a250448a3498380dc8453fd744e36"
            ],
            "layout": "IPY_MODEL_e85baad46cc646b49a762bf6d3449944"
          }
        },
        "23f4ed3dc5064995a1b48b2636c5418c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dddf0af97bf49bdacb4806a62a547c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_52192df736a4414c89114d9341e531ff",
            "value": ""
          }
        },
        "a0261e58d80042a0a741c9d6ef484231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b7ca91e6da84aaea20aa78f3d5b0943",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66d7a506fe7e40c78666ca1ff7da4ac6",
            "value": 1
          }
        },
        "e14a250448a3498380dc8453fd744e36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b3156ec60b429cb36b6134bdd2bbe2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6e1f85bb34344bc7a19f326ae3afba90",
            "value": "‚Äá16/?‚Äá[00:18&lt;00:00,‚Äá‚Äá1.30it/s]"
          }
        },
        "e85baad46cc646b49a762bf6d3449944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dddf0af97bf49bdacb4806a62a547c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52192df736a4414c89114d9341e531ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b7ca91e6da84aaea20aa78f3d5b0943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "66d7a506fe7e40c78666ca1ff7da4ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21b3156ec60b429cb36b6134bdd2bbe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e1f85bb34344bc7a19f326ae3afba90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}